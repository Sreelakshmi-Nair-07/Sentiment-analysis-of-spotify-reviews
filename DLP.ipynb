{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n",
    "print(\"Starting Spotify Reviews Thematic Analysis...\")\n",
    "print(\"This notebook will perform:\")\n",
    "print(\"1. RoBERTa sentiment analysis\")\n",
    "print(\"2. Topic modeling with LDA\")\n",
    "print(\"3. Keyword frequency analysis\")\n",
    "print(\"4. Business insights generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\Projects\\Deep_Learning-main\\spotify_reviews.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Sentiment Analysis\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: RoBERTa Sentiment Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Using cardiffnlp/twitter-roberta-base-sentiment model\")\n",
    "print(\"This will analyze sentiment of all reviews...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'content'  \n",
    "\n",
    "# Load ONLY cardiffnlp/twitter-roberta-base-sentiment model\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "print(\"Loading cardiffnlp/twitter-roberta-base-sentiment model...\")\n",
    "print(\"This may take several minutes due to model size and network...\")\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=\"./model_cache\",\n",
    "    local_files_only=False,\n",
    "    force_download=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"./model_cache\", \n",
    "    local_files_only=False,\n",
    "    force_download=False\n",
    ")\n",
    "\n",
    "print(\"RoBERTa model loaded successfully!\")\n",
    "\n",
    "# Creating pipeline\n",
    "print(\"Creating sentiment analysis pipeline\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# define function to get sentiment scores with progress tracking\n",
    "def get_sentiment_scores(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Process texts in batches to get sentiment scores.\n",
    "    Returns a list of dictionaries with labels and scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_batches = len(texts) // batch_size + (1 if len(texts) % batch_size > 0 else 0)\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{total_batches}...\")\n",
    "        \n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_texts = [str(text) if pd.notnull(text) else \"\" for text in batch_texts]\n",
    "        \n",
    "        batch_results = sentiment_pipeline(batch_texts, truncation=True, max_length=512)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Starting sentiment analysis with RoBERTa...\")\n",
    "texts = df[text_column].tolist()\n",
    "sentiment_results = get_sentiment_scores(texts)\n",
    "\n",
    "# Process results\n",
    "df['Sentiment_Label'] = [result['label'].replace('LABEL_0', 'NEGATIVE').replace('LABEL_1', 'NEUTRAL').replace('LABEL_2', 'POSITIVE') for result in sentiment_results]\n",
    "df['Sentiment_Score'] = [result['score'] for result in sentiment_results]\n",
    "\n",
    "sentiment_mapping = {'NEGATIVE': -1, 'NEUTRAL': 0, 'POSITIVE': 1}\n",
    "df['Sentiment_Numeric'] = df['Sentiment_Label'].map(sentiment_mapping)\n",
    "\n",
    "# Save results\n",
    "output_path = 'spotify_reviews_with_sentiment.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"RoBERTa sentiment analysis completed!\")\n",
    "print(\"Sample results:\")\n",
    "print(df[[text_column, 'Sentiment_Label', 'Sentiment_Score', 'Sentiment_Numeric']].head())\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df['Sentiment_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing RoBERTa generated sentiments in a file\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 2: Storing Sentiment Results\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Sentiment analysis results have been saved to:\")\n",
    "print(\"- spotify_reviews_with_sentiment.csv\")\n",
    "print(\"- Added columns: Sentiment_Label, Sentiment_Score, Sentiment_Numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if sentiment analysis was completed\n",
    "if 'Sentiment_Label' in df.columns:\n",
    "    print(\"Sentiment analysis already completed!\")\n",
    "    print(f\"Sentiment distribution:\")\n",
    "    print(df['Sentiment_Label'].value_counts())\n",
    "else:\n",
    "    print(\"Please run the sentiment analysis cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking samples\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 3: Checking Sample Data\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Displaying sample reviews with sentiment labels...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'D:\\Projects\\Deep_Learning-main\\spotify_reviews_with_sentiment.csv')\n",
    "df[['reviewId', 'content', 'score', 'at', 'Sentiment_Label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for Thematic Analysis\n",
    "# Ensure timestamp is datetime\n",
    "df['at'] = pd.to_datetime(df['at'])\n",
    "\n",
    "# Scale Sentiment_Numeric from [-1, 1] to [0, 1] (if needed for future analysis)\n",
    "df['Sentiment_Numeric_Scaled'] = (df['Sentiment_Numeric'] + 1) / 2\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['at'].min()} to {df['at'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix NLTK LookupError - Download required resources\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading required NLTK resources...\")\n",
    "\n",
    "# Download punkt_tab (newer version)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"✅ punkt_tab already available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt_tab...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Download punkt (fallback)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"✅ punkt already available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Download stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✅ stopwords already available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Download wordnet\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    print(\"✅ wordnet already available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading wordnet...\")\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "print(\"✅ All NLTK resources downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thematic Analysis - Topic Modeling and Keyword Analysis\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 4: Thematic Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This section will perform:\")\n",
    "print(\"- Topic modeling with LDA\")\n",
    "print(\"- Keyword frequency analysis\")\n",
    "print(\"- Word cloud generation\")\n",
    "print(\"- Business insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Preprocess Reviews with Better Stopwords\n",
    "print(\"=\" * 30)\n",
    "print(\"Enhanced Preprocessing Reviews\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Enhanced stopwords list - remove meaningless words\n",
    "basic_stopwords = set(stopwords.words('english'))\n",
    "additional_stopwords = {\n",
    "    'get', 'would', 'use', 'ive', 'dont', 'cant', 'wont', 'should', 'could', \n",
    "    'might', 'may', 'can', 'will', 'shall', 'must', 'need', 'want', 'like',\n",
    "    'just', 'really', 'actually', 'basically', 'literally', 'totally',\n",
    "    'thing', 'things', 'stuff', 'way', 'ways', 'time', 'times', 'day', 'days',\n",
    "    'year', 'years', 'month', 'months', 'week', 'weeks', 'hour', 'hours',\n",
    "    'minute', 'minutes', 'second', 'seconds', 'moment', 'moments',\n",
    "    'app', 'apps', 'phone', 'phones', 'device', 'devices', 'computer', 'computers',\n",
    "    'user', 'users', 'people', 'person', 'someone', 'anyone', 'everyone',\n",
    "    'something', 'anything', 'everything', 'nothing', 'somewhere', 'anywhere',\n",
    "    'everywhere', 'nowhere', 'somehow', 'anyhow', 'somewhat', 'anywhat','spotify','song'\n",
    "}\n",
    "\n",
    "all_stopwords = basic_stopwords.union(additional_stopwords)\n",
    "\n",
    "def preprocess_text_enhanced(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text with enhanced stopwords\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filter tokens with enhanced stopwords\n",
    "    tokens = [word for word in tokens if word not in all_stopwords and len(word) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing positive reviews with enhanced stopwords...\")\n",
    "positive_processed = [preprocess_text_enhanced(text) for text in positive_reviews]\n",
    "positive_processed = [text for text in positive_processed if len(text.strip()) > 10]  # Remove very short texts\n",
    "\n",
    "print(\"Preprocessing negative reviews with enhanced stopwords...\")\n",
    "negative_processed = [preprocess_text_enhanced(text) for text in negative_reviews]\n",
    "negative_processed = [text for text in negative_processed if len(text.strip()) > 10]  # Remove very short texts\n",
    "\n",
    "print(f\"Processed positive reviews: {len(positive_processed)}\")\n",
    "print(f\"Processed negative reviews: {len(negative_processed)}\")\n",
    "\n",
    "# Show sample of processed text\n",
    "print(\"\\nSample processed positive review:\")\n",
    "print(positive_processed[0][:200] + \"...\")\n",
    "print(\"\\nSample processed negative review:\")\n",
    "print(negative_processed[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Positive and Negative Reviews\n",
    "\n",
    "positive_reviews = df[df['Sentiment_Label'] == 'POSITIVE']['content'].dropna()\n",
    "negative_reviews = df[df['Sentiment_Label'] == 'NEGATIVE']['content'].dropna()\n",
    "\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Neutral reviews: {len(df[df['Sentiment_Label'] == 'NEUTRAL'])}\")\n",
    "\n",
    "# Sample reviews for analysis (to manage computational load)\n",
    "sample_size = 5000\n",
    "if len(positive_reviews) > sample_size:\n",
    "    positive_reviews = positive_reviews.sample(n=sample_size, random_state=42)\n",
    "if len(negative_reviews) > sample_size:\n",
    "    negative_reviews = negative_reviews.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"\\nAfter sampling:\")\n",
    "print(f\"Positive reviews for analysis: {len(positive_reviews)}\")\n",
    "print(f\"Negative reviews for analysis: {len(negative_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Reviews\n",
    "\n",
    "\n",
    "print(\"Preprocessing positive reviews...\")\n",
    "positive_processed = [preprocess_text_enhanced(text) for text in positive_reviews]\n",
    "positive_processed = [text for text in positive_processed if len(text.strip()) > 10]  # Remove very short texts\n",
    "\n",
    "print(\"Preprocessing negative reviews...\")\n",
    "negative_processed = [preprocess_text_enhanced(text) for text in negative_reviews]\n",
    "negative_processed = [text for text in negative_processed if len(text.strip()) > 10]  # Remove very short texts\n",
    "\n",
    "print(f\"Processed positive reviews: {len(positive_processed)}\")\n",
    "print(f\"Processed negative reviews: {len(negative_processed)}\")\n",
    "\n",
    "# Show sample of processed text\n",
    "print(\"\\nSample processed positive review:\")\n",
    "print(positive_processed[0][:200] + \"...\")\n",
    "print(\"\\nSample processed negative review:\")\n",
    "print(negative_processed[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weighted keyword frequencies using thumbsUpCount\n",
    "print(\"Analyzing weighted keyword frequencies...\")\n",
    "positive_keywords_weighted = get_weighted_keyword_frequency_from_processed(\n",
    "    positive_processed, df, 'POSITIVE', top_n=20\n",
    ")\n",
    "negative_keywords_weighted = get_weighted_keyword_frequency_from_processed(\n",
    "    negative_processed, df, 'NEGATIVE', top_n=20\n",
    ")\n",
    "\n",
    "print(\"\\n=== TOP WEIGHTED KEYWORDS IN POSITIVE REVIEWS ===\")\n",
    "for word, freq in positive_keywords_weighted:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\n=== TOP WEIGHTED KEYWORDS IN NEGATIVE REVIEWS ===\")\n",
    "for word, freq in negative_keywords_weighted:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted word clouds\n",
    "print(\"Creating weighted word clouds...\")\n",
    "create_weighted_wordcloud_from_processed(\n",
    "    positive_processed, df, 'POSITIVE',\n",
    "    \"Weighted Positive Reviews (by ThumbsUp)\", \n",
    "    \"weighted_positive_wordcloud.png\"\n",
    ")\n",
    "create_weighted_wordcloud_from_processed(\n",
    "    negative_processed, df, 'NEGATIVE',\n",
    "    \"Weighted Negative Reviews (by ThumbsUp)\", \n",
    "    \"weighted_negative_wordcloud.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comparative Visualizations\n",
    "def create_comparative_plots(positive_keywords, negative_keywords):\n",
    "    \"\"\"\n",
    "    Create comparative visualizations\n",
    "    \"\"\"\n",
    "    # Extract words and frequencies\n",
    "    pos_words = [word for word, freq in positive_keywords[:15]]\n",
    "    pos_freqs = [freq for word, freq in positive_keywords[:15]]\n",
    "    \n",
    "    neg_words = [word for word, freq in negative_keywords[:15]]\n",
    "    neg_freqs = [freq for word, freq in negative_keywords[:15]]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Positive keywords bar plot\n",
    "    ax1.barh(pos_words, pos_freqs, color='green', alpha=0.7)\n",
    "    ax1.set_title('Top Keywords in Positive Reviews', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Frequency')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Negative keywords bar plot\n",
    "    ax2.barh(neg_words, neg_freqs, color='red', alpha=0.7)\n",
    "    ax2.set_title('Top Keywords in Negative Reviews', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Frequency')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('keyword_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create comparative plots\n",
    "print(\"Creating comparative visualizations...\")\n",
    "create_comparative_plots(positive_keywords_weighted, negative_keywords_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to CSV\n",
    "def save_results_to_csv(positive_keywords, negative_keywords,positive_keywords_weighted,negative_keywords_weighted):\n",
    "    \"\"\"\n",
    "    Save thematic analysis results to CSV files\n",
    "    \"\"\"\n",
    "    # Save topics\n",
    "    pos_topics_df = pd.DataFrame([\n",
    "        {\n",
    "            'Topic_ID': topic['topic_id'] + 1,\n",
    "            'Top_Words': ', '.join(topic['words']),\n",
    "            'Top_5_Words': ', '.join(topic['words'][:5])\n",
    "        }\n",
    "        for topic in positive_topics\n",
    "    ])\n",
    "    \n",
    "    neg_topics_df = pd.DataFrame([\n",
    "        {\n",
    "            'Topic_ID': topic['topic_id'] + 1,\n",
    "            'Top_Words': ', '.join(topic['words']),\n",
    "            'Top_5_Words': ', '.join(topic['words'][:5])\n",
    "        }\n",
    "        for topic in negative_topics\n",
    "    ])\n",
    "    \n",
    "    # Save keywords\n",
    "    pos_keywords_df = pd.DataFrame(positive_keywords, columns=['Word', 'Frequency'])\n",
    "    neg_keywords_df = pd.DataFrame(negative_keywords, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    pos_topics_df.to_csv('positive_topics.csv', index=False)\n",
    "    neg_topics_df.to_csv('negative_topics.csv', index=False)\n",
    "    pos_keywords_df.to_csv('positive_keywords.csv', index=False)\n",
    "    neg_keywords_df.to_csv('negative_keywords.csv', index=False)\n",
    "    \n",
    "    print(\"Results saved to CSV files:\")\n",
    "    print(\"- positive_topics.csv\")\n",
    "    print(\"- negative_topics.csv\") \n",
    "    print(\"- positive_keywords.csv\")\n",
    "    print(\"- negative_keywords.csv\")\n",
    "\n",
    "# Save results\n",
    "save_results_to_csv(positive_keywords, negative_keywords,positive_keywords_weighted,negative_keywords_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Thematic Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THEMATIC ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis analysis has successfully:\")\n",
    "print(\"Separated positive and negative reviews\")\n",
    "print(\"Performed topic modeling using LDA\")\n",
    "print(\"✅ Analyzed keyword frequencies\")\n",
    "print(\"✅ Created word clouds and visualizations\")\n",
    "print(\"✅ Generated business insights and recommendations\")\n",
    "print(\"✅ Saved results to CSV files\")\n",
    "print(\"\\nThe analysis provides actionable insights for:\")\n",
    "print(\"• Product improvement based on negative themes\")\n",
    "print(\"• Marketing strategy based on positive themes\")\n",
    "print(\"• Customer satisfaction monitoring\")\n",
    "print(\"• Business decision making\")\n",
    "print(\"\\n🎉 Analysis Complete! Check the generated files:\")\n",
    "print(\"- positive_wordcloud.png\")\n",
    "print(\"- negative_wordcloud.png\")\n",
    "print(\"- keyword_comparison.png\")\n",
    "print(\"- positive_topics.csv\")\n",
    "print(\"- negative_topics.csv\")\n",
    "print(\"- positive_keywords.csv\")\n",
    "print(\"- negative_keywords.csv\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
